{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Session 11 Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the below is to classify days over years 2017-2018 by their corresponding mobility patterns between 10 zones in Taipei (quantified by an aggregated temporal network of subway ridership flows across the city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "TNet=pd.read_csv('https://raw.githubusercontent.com/CUSP2021ADS/Data/main/taipeiD_TNet2.csv',header=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017943</td>\n",
       "      <td>0.005415</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.008316</td>\n",
       "      <td>0.007859</td>\n",
       "      <td>0.012942</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.019543</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.008922</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.011075</td>\n",
       "      <td>0.005073</td>\n",
       "      <td>0.012708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021283</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.007803</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.011185</td>\n",
       "      <td>0.019044</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.009618</td>\n",
       "      <td>0.005946</td>\n",
       "      <td>0.013709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.028988</td>\n",
       "      <td>0.006511</td>\n",
       "      <td>0.005591</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>0.015878</td>\n",
       "      <td>0.010973</td>\n",
       "      <td>0.015768</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.007385</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.008791</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.016301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029534</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>0.013017</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>0.016098</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.015815</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>0.005443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>0.007255</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.008729</td>\n",
       "      <td>0.010296</td>\n",
       "      <td>0.016437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.029333</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.013098</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.016358</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.015677</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.007269</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>0.010381</td>\n",
       "      <td>0.016914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.017943  0.005415  0.003590  0.008316  0.007859  0.012942  0.012196   \n",
       "1  0.021283  0.005215  0.003530  0.009359  0.007803  0.014288  0.011185   \n",
       "2  0.028988  0.006511  0.005591  0.012970  0.007816  0.015878  0.010973   \n",
       "3  0.029534  0.006471  0.005615  0.013017  0.007717  0.016098  0.011182   \n",
       "4  0.029333  0.006525  0.005727  0.013098  0.007692  0.016358  0.011000   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0  0.019543  0.001196  0.003327  ...  0.002529  0.001533  0.001860  0.002375   \n",
       "1  0.019044  0.001382  0.003499  ...  0.002803  0.001757  0.001783  0.002549   \n",
       "2  0.015768  0.002252  0.005388  ...  0.004649  0.002555  0.002672  0.004291   \n",
       "3  0.015815  0.002325  0.005443  ...  0.004611  0.002473  0.002636  0.004195   \n",
       "4  0.015677  0.002344  0.005527  ...  0.004694  0.002515  0.002677  0.004222   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.005408  0.008922  0.003945  0.011075  0.005073  0.012708  \n",
       "1  0.005515  0.009650  0.003596  0.009618  0.005946  0.013709  \n",
       "2  0.007385  0.009558  0.004293  0.008791  0.010040  0.016301  \n",
       "3  0.007255  0.009487  0.004316  0.008729  0.010296  0.016437  \n",
       "4  0.007269  0.009921  0.004387  0.008923  0.010381  0.016914  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TNet.head() \n",
    "#each row represents a 10x10 adjacency matrix of the normalized Taipei subway mobility network between 10 zones flattened into a 100x1 row corresponding to a single day\n",
    "#days start at jan-1-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to an array and scale the data\n",
    "X=np.array(TNet);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(669, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define day of the week corresponding to each day of observation; 0-Sunday, 1-Monday,...,6-Saturday\n",
    "y=np.array(range(669))%7; y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc=np_utils.to_categorical(y) #get categorical binary variables isSunday, isMonday,...\n",
    "yc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X[400:,:]; X_train=X[:400,:]; #split the data into training and test\n",
    "y_test=yc[400:,:]; y_train=yc[:400,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Classify weekdays/weekends\n",
    "Label the rows with ones for weekends, zeros for weekdays.\n",
    "Train a neural network with 4 layers of 30,10,3 and 1 (output) neurons over the training sample against this label, evaluating its performance over the test sample. Report the acheived accuracy (categorical) over the test sample\n",
    "\n",
    "First three layers use relu activation function, last one - sigmoid.\n",
    "Use loss='binary_crossentropy', optimizer='adam', 100 epochs, batch_size=20. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Classify all days of the week\n",
    "Train a neural network against the origial categorical label. Use 5 layers of 40,15,5 and 7 (outputs, representing probabilities for a current input to correspond to each of the weekdays) neurons over the training sample, evaluating its performance over the test sample (use 'categorical_accurary'). Report the acheived accuracy (categorical) over the test sample.\n",
    "\n",
    "First three layers use relu activation function, last one - sigmoid.\n",
    "Use loss='binary_crossentropy', optimizer='adam', 200 epochs, batch_size=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 2\n",
    "\n",
    "Use the same datasets as in the Advanced NN lab and further train NN with different architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Try the facial recognition task from NN lab 2 with different model specifications and report model accuracy:\n",
    "1. Try different dropouts - 0.2, 0.3, 0.4\n",
    "2. Try different convolution windows: (3$\\times$3, 10$\\times$10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people \n",
    "from keras.datasets import mnist\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person = 70, resize = 0.4) \n",
    "  \n",
    "# the images arrays to find the shapes (for plotting) \n",
    "n_samples, h, w = lfw_people.images.shape \n",
    "  \n",
    "# Instead of providing 2D data, X has data already in the form  of a vector that \n",
    "# is required in this approach. \n",
    "X = lfw_people.data \n",
    "n_features = X.shape[1] \n",
    "\n",
    "y = lfw_people.target \n",
    "target_names = lfw_people.target_names \n",
    "n_classes = target_names.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "1. Try different word count in the next-word prediction task with a)8 and b)12 previous words and report accuracy for each. Use the same Sherlock data.\n",
    "\n",
    "2. Try different LSTM architectures and comment on model performace for each. \n",
    "    - Add a dropout layer with 0.25\n",
    "    - Change number of neurons to a) 64 b) 256 neurons\n",
    "    \n",
    "    Train for 10 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 581889\n"
     ]
    }
   ],
   "source": [
    "path = 'sherlock.txt'\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
